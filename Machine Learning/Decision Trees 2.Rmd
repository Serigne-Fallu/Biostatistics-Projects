---
title: "Machine Learning: Decision Tree 2"
author: "Serigne Fallou MBacke NGOM"
date: "2023-11-22"
output: html_document
---

```{r, include=FALSE}
library(reticulate)
use_virtualenv("my-python")
#virtualenv_install(envname = "my-python", "numpy", ignore_installed = FALSE, pip_options = character())
```


### Reading Data
```{python}
import numpy as np
import pandas as pd

data = pd.read_csv('cancerdata1.csv')
data.loc[np.r_[0:3, 51:53, 101:103], :]
```


### Data Check (Missing Values, Label Check)
- Check the dataset to make sure no data is missing and Check the class labels;
- Use data_found as a dummy variable to determine whether to print missing value information
```{python}
def verify_dataset(data):
    data_found = 1
    for each_column in data.columns:
        if data[each_column].isnull().any():
            print("Data missing in Column " + each_column)
            data_found = 0
            quit()

        if data_found == 1:
            print("Dataset is complete. No missing value")

        return

verify_dataset(data)
```

### Create a Training & Testing Set
The data set of 150 inds is divided into 70% for training and 30% for testing (3-fold cross validation):
- Splitting The Datase in training and testing;
- Use the .sample() function to scramble the data set;
- Determine the integer location (iloc) from beginning of array (:) to 0.7*150 and do a ”cleanup” with a reset call;
- Call split_dataset_test_train and check data sets.
```{python}
def split_dataset_test_train(data):
    data = data.sample(frac=1).reset_index(drop=True)
    training_data = data.iloc[:int(0.7 * len(data))].reset_index(drop=True)
    testing_data = data.iloc[int(0.7 * len(data)):].reset_index(drop=True)
    return [training_data, testing_data]

testtrain = split_dataset_test_train(data)
print(testtrain)
```

### Calculate gini index for a given split:
The Gini index measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen.
Gini index is between 0 and 1.
```{python}
def gini_index(data, target_col):
    elements, counts = np.unique(data[target_col], return_counts = True)
    total_counts = sum(counts)
    sum_prob = 0.0
    for i in range (elements.size):
        prob_i = counts[i] / total_counts
        sum_prob = sum_prob + prob_i * prob_i

    gini_index= 1 - sum_prob
    return gini_index
```

### Information gain
This function measures the reduction of the Gini index (a measure of data impurity) by dividing the dataset into two parts based on a specific feature and a threshold value. Information gain is used in decision tree algorithms to select the best feature and threshold value to split the data.
```{python}
def information_gain(data, target_col, threshold, target_class = "Results"):
    total_gini_index = gini_index(data, "Results")
    data_left = data[data[target_col] < threshold]
    data_right = data[data[target_col] >= threshold]
    gini_index_after_split = data_left.shape[0]/ data.shape[0] * gini_index(data_left, "Results") + data_right.shape[0]/data.shape[0] * gini_index(data_right, "Results")
    info_gain = total_gini_index - gini_index_after_split
    return info_gain
```

### Establish optimal splits based on the best features, best cutoffs, and best information gains
```{python}
def selectBestFeatureAndCutoff(data, target_class = "Results"):
    featureList = list(data)[0:4]
    best_feature = "None"
    best_cutoff = 0.0
    best_info_gain = 0.0
    for feature in featureList:
        max_value = data[feature].max()
        min_value = data[feature].min()
        for cutoff in np.arange(min_value, max_value, 0.1):
            if best_info_gain < information_gain(data, feature, cutoff):
                best_info_gain = information_gain(data, feature, cutoff)
                best_cutoff = cutoff
                best_feature = feature

    return [best_feature, best_cutoff, best_info_gain]
```

### Define the decision tree root (ie the first node), create the associated recursive splitting function, and create the associated prediction function
```{python}
class Node:
    def __init__(self, feature, cut_off, label = None, is_leaf = False):
        self.feature = feature
        self.cut_off = cut_off
        self.left_child = None
        self.right_child = None
        self.is_leaf = is_leaf
        self.label = label
        #print("node's label: ")
        #print(self.label)
        
class DTree:
    # method to train a decision tree
    def train(self, data):
        self.root = self.build_tree(data)

    # method to build decision tree
    def build_tree(self, data):
        best_feature, best_cutoff, best_info_gain = selectBestFeatureAndCutoff(data)
        # if all data has the same label , we are at a leaf node
        if len(np.unique(data["Results"])) == 1:
            #print(data["variety"].iloc[0])
            return Node(best_feature, best_cutoff, data["Results"].iloc[0], True)

        # if we are not the leaf
        # first lets split data
        data_left = data[data[best_feature] < best_cutoff]
        data_right = data[data[best_feature] >= best_cutoff]

        #build current node
        current_node = Node(best_feature, best_cutoff)
        #add left node
        current_node.left_child = self.build_tree(data_left)
        #add right node
        current_node.right_child = self.build_tree(data_right)

        return current_node
 # Make a prediction with a decision tree
    def predict(self, data):
        current_node = self.root
        while(True):

            # if we are at the leaf node , return label
            if current_node.is_leaf == True:
                return current_node.label
            # otherwise we need figure out where to go next
            feature = current_node.feature
            cutoff = current_node.cut_off
            if data[feature]  < cutoff:
                current_node = current_node.left_child
            else:
                current_node = current_node.right_child

```

### Train the decision tree
```{python}
d_tree = DTree()
training_data = testtrain[0]
d_tree.train(training_data)
```

### Define the confusion matrix
```{python}
def print_ConfusionMatrix(result):
    count_SS = result[0]
    count_SVi = result[1]
    count_SVe = result[2]
    count_ViVi = result[3]
    count_ViVe = result[4]
    count_ViS = result[5]
    count_VeVe = result[6]
    count_VeVi = result[7]
    count_VeS = result[8]
    count_total_T =  result[9]
    count_total_F =  result[10]

    print ("True - Cured, Predicted - Cured : count_SS = ",  count_SS)
    print ("True - Cured, Predicted - Recurrence: count_SVi = ",  count_SVi)
    print ("True - Cured, Predicted - Dead: count_SVe = ",  count_SVe)

    print ("True - Dead, Predicted - Dead: count_ViVi = ",  count_ViVi)
    print ("True - Dead, Predicted - Recurrence: count_ViVe = ",  count_ViVe)
    print ("True - Dead, Predicted - Cured: Cured = ",  count_ViS)

    print ("True - Recurrence, Predicted - Recurrence: count_VeVe = ",  count_VeVe)
    print ("True - Recurrence, Predicted - Dead: count_VeVi = ",  count_VeVi)
    print ("True - Recurrence, Predicted - Cured:count_VeS = ",  count_VeS)

    print ("count_total_T = ",  count_total_T)
    print ("count_total_F = ",  count_total_F)


    print ("1) count_SS / (count_SS + count_ViS + count_VeS) = ", count_SS / (count_SS + count_ViS + count_VeS))
    if (count_SS + count_ViS + count_VeS)!=0:
      count_SS_ratio=count_SS / (count_SS + count_ViS + count_VeS)
    else:
      count_SS_ratio=0

    print ("2) count_SVi / (count_SVi + count_ViVi + count_VeVi) = ", count_SVi / (count_SVi + count_ViVi + count_VeVi))
    print ("3) count_SVe / (count_SVe + count_ViVe + count_VeVe) = ", count_SVe / (count_SVe + count_ViVe + count_VeVe))


    print ("4) count_ViS / (count_SS + count_ViS + count_VeS) = ", count_ViS / (count_SS + count_ViS + count_VeS))
    print ("5) count_ViVi / (count_SVi + count_ViVi + count_VeVi) = ", count_ViVi / (count_SVi + count_ViVi + count_VeVi))
    print ("6) count_ViVe / (count_SVe + count_ViVe + count_VeVe) = ", count_ViVe / (count_SVe + count_ViVe + count_VeVe))


    print ("7) count_VeS / (count_SS + count_ViS + count_VeS) = ", count_VeS / (count_SS + count_ViS + count_VeS))
    print ("8) count_VeVi / (count_SVi + count_ViVi + count_VeVi) = ", count_VeVi / (count_SVi + count_ViVi + count_VeVi))
    print ("9) count_VeVe / (count_SVe + count_ViVe + count_VeVe) = ", count_VeVe / (count_SVe + count_ViVe + count_VeVe))


    data = {"predict\Observe": ["Cured (predict)", "Recurrence (predict)", "Deceased (predict)"],
            "Cured (observed)": [ count_SS_ratio, count_SVi / (count_SVi + count_ViVi + count_VeVi), count_SVe / (count_SVe + count_ViVe + count_VeVe)],
            "Recurrence (observed)": [count_ViS / (count_SS + count_ViS + count_VeS), count_ViVi / (count_SVi + count_ViVi + count_VeVi), count_ViVe / (count_SVe + count_ViVe + count_VeVe)],
            "Deceased (observed)": [count_VeS / (count_SS + count_ViS + count_VeS), count_VeVi / (count_SVi + count_ViVi + count_VeVi), count_VeVe / (count_SVe + count_ViVe + count_VeVe)]
            }

    output = pd.DataFrame(data, columns = ["predict\Observe", "Cured (observed)", "Recurrence (observed)", "Deceased (observed)"])
    return output
```

### Create the confusion matrix
```{python}
def predict_batch(data):
    d_tree = DTree()
    d_tree.train(training_data)
    count_SS = 0
    count_SVi = 0
    count_SVe = 0
    count_ViVi = 0
    count_ViS = 0
    count_ViVe = 0
    count_VeVe = 0
    count_VeS = 0
    count_VeVi = 0
    count_total_T = 0
    count_total_F  = 0

    for i in range (data.shape[0]):
        instance = data.iloc[i]
        true_label = instance["Results"]
        predict_label = d_tree.predict(data.iloc[i])
        print (i, ") true_label  = ", true_label , "predict_label  = ", predict_label )
        if true_label == predict_label:
            count_total_T = count_total_T + 1
            if true_label == "Cured":
                count_SS = count_SS + 1
            elif true_label == "Dead":
                count_ViVi = count_ViVi + 1
            elif true_label == "Recurrence":
                count_VeVe = count_VeVe + 1
        else:
            count_total_F = count_total_F + 1
            if true_label == "Cured" and predict_label == "Recurrence":
                count_SVi = count_SVi + 1
            elif true_label == "Cured" and predict_label == "Dead":
                count_SVe = count_SVe + 1
            elif true_label == "Dead" and predict_label == "Recurrence":
                count_VeVi = count_VeVi + 1
            elif true_label == "Dead" and predict_label == "Cured":
                count_VeS = count_VeS + 1
            elif true_label == "Recurrence" and predict_label == "Dead":
                count_ViVe = count_ViVe + 1
            elif true_label == "Recurrence" and predict_label == "Cured":
                count_ViS = count_ViS + 1

    return [count_SS, count_SVi, count_SVe, count_ViVi, count_ViVe, count_ViS, count_VeVe, count_VeVi, count_VeS, count_total_T, count_total_F]

```

### Look at the confusion matrix for training data
```{python}
training_data = testtrain[0]
print ("training_data = ", training_data)
predict_batch_results=predict_batch(training_data)
print ("predict_batch_results = ", predict_batch_results)
print_ConfusionMatrix(predict_batch(training_data))
```

### Look at the confusion matrix for testing data
```{python}
testing_data = testtrain[1]
print_ConfusionMatrix(predict_batch(testing_data))

```


### Make predictions
```{python}
# method that run prediction
def predict(d_tree, ESR1, PGR, BCL2, NAT1):
    test_data = pd.Series([ESR1, PGR, BCL2, NAT1], index = ['ESR1', 'PGR', 'BCL2', 'NAT1'])
    return d_tree.predict(test_data)
```

- Predict a new ind with [ESR1=1 , PGR=1 , BCL2=1 , NAT1=1]
```{python}
predict_features = [1, 1, 1, 1]

result_category = predict(d_tree, predict_features[0], predict_features[1], predict_features[2], predict_features[3])

print("This patient is ", result_category)
```


- Predict a new ind with [ESR1=1 , PGR=2 , BCL2=3 , NAT1=4]
```{python}
predict_features = [1, 2, 3, 4]

result_category = predict(d_tree, predict_features[0], predict_features[1], predict_features[2], predict_features[3])

print("This patient is ", result_category)
```